# Transformer Chatbot ðŸ¤–
A simple chatbot built using the vanilla Transformer architecture.
This project implements a basic chatbot using the original Transformer architecture introduced by Google in the paper ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762).

> ðŸ§  This work is inspired by Google's groundbreaking research on attention mechanisms and sequence modeling.


<p align="center">
  <img src="Screenshot 2025-07-03 031652.png" alt="Sample Gesture" width="900"/>
</p>

## ðŸ”§ Model Overview
The Transformer uses:
- Multi-head self-attention
- Positional encoding to preserve word order
- Encoder-decoder blocks with residual connections

### Key Parameters
- `d_model`: Embedding size (e.g. 512)
- `n_heads`: Number of attention heads (e.g. 8)
- `num_layers`: Layers in encoder/decoder (e.g. 4)
- `dropout`: Regularization to prevent overfitting

## ðŸ§ª Pipeline
1. **Preprocessing** â€“ Tokenize and pad sentence pairs
2. **Training** â€“ Minimize cross-entropy between predicted and target tokens
3. **Inference** â€“ Generate replies using greedy decoding

## ðŸ›  Requirements
- Python 3.8+
- PyTorch
- NumPy
- Jupyter Notebook

## ðŸš€ Usage
Open `transformer_chatbot.ipynb` and run all cells to train and chat with the model.

---

