# Transformer Chatbot ðŸ¤–

A simple chatbot built using the vanilla Transformer architecture (Vaswani et al., 2017). It uses the encoder-decoder attention mechanism to learn basic conversational patterns from paired text data.

## Features
- Encoder-Decoder Transformer (vanilla version)
- Positional Encoding
- Trained on basic dialogue pairs
- Inference with greedy decoding

## ðŸ”§ Model Overview
The Transformer uses:
- Multi-head self-attention
- Positional encodings to retain word order
- Feed-forward layers and residual connections

### Key Parameters
- `d_model`: Embedding size (512)
- `n_heads`: Number of attention heads (8)
- `num_layers`: Layers in encoder/decoder (4)
- `dropout`: Regularization to prevent overfitting

## ðŸ§ª Pipeline
1. **Preprocessing** â€“ Tokenize and pad sentence pairs
2. **Training** â€“ Minimize cross-entropy between predicted and target tokens
3. **Inference** â€“ Generate replies token-by-token using greedy decoding

## ðŸ›  Requirements
- Python 3.8+
- PyTorch
- NumPy
- Jupyter Notebook

## ðŸš€ Usage
Open `transformer_chatbot.ipynb` and run all cells to train and interact with the chatbot.

---

