# Transformer Chatbot ðŸ¤–
A simple chatbot built using the vanilla Transformer architecture.
This project implements a basic chatbot using the original Transformer architecture introduced by Google in the paper ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762).

> ðŸ§  This work is inspired by Google's groundbreaking research on attention mechanisms and sequence modeling.

## ðŸ”§ Model Overview
The Transformer uses:
- Multi-head self-attention
- Positional encoding to preserve word order
- Encoder-decoder blocks with residual connections

### Key Parameters
- `d_model`: Embedding size
- `n_heads`: Number of attention heads
- `num_layers`: Number of encoder/decoder layers
- `dropout`: Dropout rate for regularization

## ðŸ§ª Pipeline
1. **Preprocessing** â€“ Tokenize and pad sentence pairs
2. **Training** â€“ Minimize cross-entropy between predicted and target tokens
3. **Inference** â€“ Generate replies using greedy decoding

## ðŸ›  Requirements
- Python 3.8+
- PyTorch
- NumPy
- Jupyter Notebook

## ðŸš€ Usage
Open `transformer_chatbot.ipynb` and run all cells to train and chat with the model.

---

